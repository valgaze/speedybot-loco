import { Speedybot, Config } from "speedybot-mini";
const botConfig: Config = {
  locales: {
    es: {
      greetings: {
        welcome: "hola!!",
      },
    },
    cn: {
      greetings: {
        welcome: "你好",
      },
    },
  },
};

// In a production environment use a secrets manager to pass in token

// 1) Initialize your bot w/ config
const CultureBot = new Speedybot(botConfig);

// 2) Export your bot
export default CultureBot;

// 3) Do whatever you want!

CultureBot.contains(["ping", "pong"], ($bot, msg) => {
  const { text } = msg;
  if (text === "ping") {
    $bot.send("pong");
  } else if (text === "pong") {
    $bot.send("ping");
  }
});

CultureBot.exact("clear", ($bot) => $bot.clearScreen());

// Special Handlers

CultureBot.nlu(async ($bot, msg, api) => {
  try {
    const modelConfig: CreateCompletionRequest = {
      model: "text-davinci-001",
      temperature: 0.85,
    };
    const response = await open_ai(
      msg.text,
      modelConfig,
      $bot.secrets.openai,
      api
    );
    const [choice] = response.choices;
    const { text } = choice;
    const SHOW_WARNING = true;

    if (SHOW_WARNING) {
      const warning = "[Warning: response generated by languge model]";
      $bot.send(warning + text);
    } else {
      // send without warning prefix
      $bot.send(text);
    }
  } catch (e) {
    console.log("[ERROR]", e);
    $bot.send(`There was a catastrophic error: ${e.message}`);
  }
});

// onSubmit
CultureBot.onSubmit(($bot, msg) => {
  $bot.send(`You sent ${JSON.stringify(msg.data.inputs)}`);
});

// Runs on file upload, can pass bytes to 3rd-party service
CultureBot.onFile(async ($bot, msg, fileData) => {
  $bot.send(
    `You uploaded '${fileData.fileName}' (${fileData.extension}, ${fileData.type})`
  );
  const snippetable = ["json", "txt", "csv"];
  if (snippetable.includes(fileData.extension)) {
    // Actual data (markdown preview available on fileData.markdownSnippet)
    $bot.sendSnippet(fileData.data);
  }
}).config({ matchText: true });

// interface from here: https://github.com/openai/openai-node/blob/eaade749f2ead531d2ac9a2015184f7b6418a581/api.ts#L29
interface CreateCompletionRequest {
  model: string;
  prompt?: string | null;
  suffix?: string | null;
  max_tokens?: number | null;
  temperature?: number | null;
  top_p?: number | null;
  n?: number | null;
  stream?: boolean | null;
  logprobs?: number | null;
  echo?: boolean | null;
  stop?: string | null;
  presence_penalty?: number | null;
  best_of?: number | null;
  frequency_penalty?: number | null;
  presence_based_on_prefix?: boolean | null;
}

export type TextCompletionResponse = {
  warning: string;
  id: string;
  object: string;
  created: number;
  model: string;
  choices: {
    text: string;
    index: number;
    logprobs: any;
    finish_reason: string;
  }[];
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
};

export const open_ai = async (
  query: string,
  opts: Partial<CreateCompletionRequest> = {},
  apiKey,
  api
): Promise<TextCompletionResponse> => {
  const warning = "[Warning: this is generated by a language model]";
  const prompt_prefix = `Prefix your responses to following question with what you think the smartest, most capable conversation agent ever made would say-
`;
  // Set up the request options
  const options = {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    token: apiKey,
  };
  const body = Object.assign(
    {
      model: "text-davinci-001", // 001 has 175b parameters
      max_tokens: 256,
      temperature: 0.7,
    },
    opts,
    {
      prompt: opts.prompt
        ? `${opts.prompt}${query}`
        : `${prompt_prefix}${query}`,
    }
  );
  const res = await api("https://api.openai.com/v1/completions", body, options);
  const json = await res.json();
  return { ...json, warning } as TextCompletionResponse;
};
